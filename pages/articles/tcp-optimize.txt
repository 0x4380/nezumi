====== тонкая настройка параметров TCP/IP\\ под толстые каналы ======
<sub>{{TCP-optimize.odt|Original file}}</sub>

крис касперски, ака мыщъх, a.k.a. nezumi, a.k.a. souriz, no-email

**пропускная способность локальных сетей и Интернет-каналов неуклонно растет, однако, вместе с ней растут и потребности, вызывающие естественное желание выжать из ****TCP/IP ****стека максимум возможного, чем мы сейчас, собственно, и займемся, главным образом акцентируя внимания на ****Windows Server 2003****, хотя описанные технология оптимизации справедливы и для рабочих станций, собранных на базе ****W2K/XP**

{{INLINETOC}}
===== введение =====

По поводу кручения настроек TCP/IP существуют два диаметрально противоположных мнения: многие администраторы (а вместе с ними и авторы популярных книг!) считают, что разработчики уже сделали все, что нужно и любое вмешательство в этот четко отлаженный механизм может только навредить. В то же самое время в Интернете валяется множество руководств, обещающих если не путевку в рай, то радикальное увеличение производительности ценою изменения пары-тройки ключей в системном реестре.

Истина, как водится, где-то посередине. Операционные системы уже давно научились автоматически распознавать тип подключения, выбирая соответствующий ему набор настроек по умолчанию. Адаптивные алгоритмы динамически подстраиваются под характеристики канала и неквалифицированные "указания" пользователя, действительно, только мешают. Однако, адаптивным алгоритмам свойственно ошибаться, а настройки по умолчанию далеко не всегда соответствуют характеристикам конкретно взятых каналов связи, разброс которых настолько велик, что просто колоссален.

Какой прирост производительности может дать оптимизация параметров TCP/IP, при условии, что она выполнена правильно? Зависит от того, насколько настройки по умолчанию близки к свойствам используемого канала. В среднем, следует ожидать 20%-30% выигрыша, однако, в "клинических" случаях скорость увеличивается в несколько раз!

===== прежде чем приступать к оптимизации =====

Вместо того, чтобы засучив рукава, с первых же строк бросаться в бой, лучше сперва покурить и подумать. Допустим, мы имеем 10 мегабитный канал и скачиваем/раздаем файлы с превалирующей скоростью порядка мегабайта в секунду. Понятно, что никакими ухищрениями нам не удастся поднять производительность на сколь ни будь заметную величину. Так стоит ли возиться?! К тому же, достаточно большое количество администраторов умышленно ущемляет отдачу в районе 50-100 Кбайт/с, предотвращая перегрузку сети. Какая уж тут оптимизация…

Другое дело, если наблюдаемая пропускная способность составляет менее 2/3 от заявленной аплинком. Тут уже без оптимизации никак не обойтись! Однако, помимо TCP/IP стека, за производительность отвечают и другие системные компоненты, например, процессор. При большом количестве одновременно установленных соединений, загрузка ЦП может достигать 100%, особенно, с учетом того, что в дешевом сетевом оборудовании подсчет контрольных сумм пакетов реализован на программном, а не аппаратном (как у дорогих моделей) уровне.

Еще одна виновница — видео-карта, надолго захватывающая шину безо всяких видимых причин, в результате чего все остальные периферийные устройства садятся на голодный паек и скорость ввода/вывода (в том числе и сетевого) многократно снижается. Обновление драйверов или отключение всех "агрессивных" настроек видео-карты обычно решают проблему даже без обращения к стеку TCP/IP.

Так же не стоит забывать и о том, что чрезмерная фрагментация дискового пространства существенно замедляет скорость отдачи/приема файлов, что является одной из основных причин замедления загрузок web-страничек у конечных пользователей.

В общем, прежде, чем лезть в TCP/IP стек, следует убедиться, что все остальные возможные причины устранены, и узким местом являются именно настройки сетевых протоколов, а не что-то иное (//**внимание**////: "убедиться" это совсем не тоже самое, что "убедить себя"//).

===== MTU + MSS = ??? =====

**MTU** (//**M**////aximum ////**T**////ransmission ////**U**////nit// – Максимальный [размер] Передаваемого Пакета), вероятно, самый известный параметр TCP/IP, рекомендации по настройке которого можно встретить практически в любой статье по оптимизации TCP/IP. Сотни утилит предлагают свои услуги по определению предельно точного значения, но, увы, обещанного увеличения производительности как-то не достигается.

MTU задет наибольший возможный размер _отправляемого_ IP-пакета (вместе с заголовком), нарезая отправляемые данные на порции фиксированного размера. Чем больше MTU, нем ниже накладные расходы на передачу служебной информации, а, значит, выше "КПД" канала. С другой стороны, маршрутизаторы сваливают пакеты, поступающие от разных узлов, в общую очередь и потому гораздо выгоднее отправить один большой пакет, чем два маленьких, причем, чем сильнее загружен маршрутизатор, тем больший выигрыш мы получим.

{{tcp-optimize_Image_0.png}}

**Рисунок 1 MTU и MSS**

Так в чем же дело?! Выкручиваем MTU до предела и… скорость падает до нуля. Почему? Причина в том, что с ростом размера пакетов увеличивается и время, необходимое для их повторной передачи, в том случае если пакет потерян или искажен. К тому же, промежуточные узлы имеют свои собственные настройки, и если размер передаваемого пакета, превышает текущий MTU, пакет разрезается на два или более пакетов, (т. е. фрагментируется) и эти фрагменты собираются воедино только на узле-приемнике, в результате чего пропускная способность уменьшается. Причем, если MTU узла отправителя лишь чуть-чуть превышает MTU промежуточного узла, то второй пакет состоит практически из одного заголовка, в результате чего зависимость скорость передачи от размера превращается в характерную пилообразную кривую (см. рис. 2).

Значения MTU, используемые Windows Server 2003 по умолчанию, приведены в таблице 1, однако, при желании их можно изменить.

{{tcp-optimize_Image_1.png}}

**Рисунок 2 зависимость скорости передачи данных от размера MTU (по данным http://member.nifty.ne.jp/oso/faq.mtu-faq.html)**

Запускам утилиту "Редактора Реестра", и открываем в ней следующий раздел: //HKLM\SYSTEM\CurrentControlSet\Services\Tcpip\Parameters\Interfaces\interfaceGUID//.Видим там параметр **MTU** типа DWORD (а если не видим, то создаем) и вводим размер в байтах (0xFFFFFFFF означает "использовать значение MTU по умолчанию). Интерфейсы заданы GUID-идентификаторами и обычно их бывает намного больше одного. Как среди них найти интерфейс кабельногомодема или конкретной сетевой карты? Да очень просто — по IP-адресу!

{{tcp-optimize_Image_2.png}}

**Рисунок 3 тонкая настройка TCP/IP параметров через "Редактор Реестра"**

Существует возможность автоматического определения маршрута, по которому пакеты с заданным MTU проходят без фрагментации (параметр **EnablePMTUDiscovery** типа DWORD, находящийся в той же ветви реестра, что и MTU. значение "1" — включает данную функцию, "0" – выключает). Однако, многие администраторы промежуточных узлом по соображениям "безопасности" блокируют отправку ICMP-сообщений и узел-отправитель остается в полном неведении относительно факта фрагментации. Специально для обнаружения таких вот "неправильных" маршрутизаторов (прозванных "черными дырами" или, по-английски, Black Hole), Windows поддерживает специальный алгоритм, управляемый параметром **EnablePMTUDiscovery** (во всем аналогичным EnablePMTUDiscovery).

{{tcp-optimize_Image_3.png}}

**Рисунок 4 "черными дырами" называют маршрутизаторы, не отправляющие ICMP-сообщения о факте фрагментации ретранслируемого пакета, что создает большие проблемы при попытке определения оптимального значения MTU**

В подавляющем большинстве случаев использование опций EnablePMTUDiscovery и EnablePMTUDiscovery приводит к снижению производительности, и значение MTU лучше выбирать, отталкиваясь от таблицы 2 или действовать методом перебора.

Еще один параметр — **MSS** (//**M**////aximum ////**S**////egment ////**S**////ize// – Максимальный Размер Сегмента) отвечает за максимальный размер передаваемых данных за вычетом длины заголовка IP-пакета (см. рис. 1). Трогать его не следует, да и Windows это все равно не позволяет. В общем случае, MSS = MTU — 40 байт.

|параметр|канал с скоростью <128 Kbps|канал со скоростью >= 128|
|MTU (Maximum Transmission Unit)|576|1500|
|MSS (Maximum Segment Size)|536|1460|

**Таблица 1 значения MTU и MSS по умолчанию в Microsoft Windows Server 2003**

|**MTU (байт)**|**протокол**|**нормативный RFC**|
|576|по умолчанию|879|
|1500|PPP по умолчанию|1134|
|296|PPP (low relay)|1144|
|1500|Ethernet|895|
|1006|SLIP|1055|
|1492|PPPOE|2516|

**Таблица 2 значение MTU, автоматически выбираемые Microsoft Windows Server 2003 в зависимости от типа подключения**

===== TCP Receive Window =====

Размер TCP-окна — малоизвестный, но чрезвычайно важный (в плане производительности) параметр, способный увеличить пропускную способность в несколько раз. Рассмотрим два узла "A"и "B" и заставим узел "A"передавать узлу "B"данные, разбитые на сегменты, размер которых (как уже говорилось) определяется параметром MSS. Протокол TCP работает с установкой соединения, что обязывает его отправлять уведомления об успешно принятых сегментах. Неподтвержденные сегменты спустя некоторое время передаются узлом "A"вновь.

Промежуток времени между отправкой пакета и его получением называется латентностью (latency) и эта латентность в зависимости от типа и загруженности сети варьируется от 20 ms (и менее) до 100 ms (и более). Легко посчитать, что если бы подтверждался _каждый_ сегмент, до даже в низколатентной сети реальная скорость передачи заметно отставала от ее реальных возможностей и была равна MTU/(2*latency), что образует предел в 6 мегабит/сек, _независящий_ от пропускной способности. Кошмар! Ну как дальше жить?!

Вот потому-то, создатели TCP/IP и разрешили узлу "A"отправлять более одного сегмента, не дожидаясь подтверждения. Максимальное количество сегментов, которые можно передать до прихода подтверждения и называется размером TCP-окна (процесс передачи хорошо проиллюстрированном на анимированном gif'e: http://cable-dsl.home.att.net/rwinanim.htm). Почему этот параметр так важен для достижения наибольшей производительности?

Допустим, мы имеем 10-мегабитный канал и передаем 7 сегментов по 1460 байт каждый, потратив на этого 8 ms. Если латентность составляет 100 ms, то… 100 ms + 92 ms == 192 ms. Мы, как идиоты, ждем подтверждения целых 192 ms и 96% времени узел "А" проводит в бездействии, используя лишь 4% пропускной способности канала. Это, конечно, крайний случай, но все-таки не настолько далекий от истины, как можно было бы подумать.

В процессе установки соединения, узел "A"предлагает узлу "B"установить размер окна, равный 16 Кбайтам (значение по умолчанию, прописанное в параметре реестре **ТсрWindowSize**, которое при желании можно изменить). Размер окна всегда округляется до целого количества сегментов (см. параметр MSS).

Если размер окна превышает 64 Кбайт, система активирует алгоритм автоматического масштабирования, который, впрочем, работает только в том случае, если узел B так же поддерживает этот механизм, поэтому, лучше задавать размер TCP-окна вручную, руководствуясь таблицей 3. (Однако, следует помнить, что слишком большое окно, забивает канал пакетами, вызывая перегрузку сети, препятствующую пересылке уведомлений, в результате чего производительность падает).

| **минимально необходимый размер ****TCP-****окна**|||||||||
| |скорость канала в (Килобит/сек)|||||||
| ::: | ::: |500|1000|1500|2000|2500|
| латентность канала\\ (ms)|50|2K|5K|7K|10K|12K|
| ::: |100|5K|10K|15K|20K|24K|
| ::: |150|7K|15K|22K|29K|37K|
| ::: |200|10K|20K|29K|39K|49K|
| ::: |250|12K|24K|37K|49K|61K|
| Windows 9x/NT\\ по умолчанию|8K|||||||
| Windows Me/2000/XP\\ Server 2003 по умолчанию|**скорость канала**|||||||
| ::: | ::: | < 1 Мегабит/сек|100 мегабит/сек| > 100 Мегабит/сек||
| ::: | ::: |8 KB|17 KB|64 KB||
|рекомендуемые значения| 32-63K|||||||

**Таблица 3 рекомендуемые значения размера TCP-окна в зависимости от характеристик канала (по данным http://cable-dsl.home.att.net/)**

===== >>> врезка один за всех — все за одного! =====

Если клиенты локальной сети работают через Proxy сервер, то для достижения максимальной производительности, достаточно изменить размер TCP-окна непосредственно на самом сервере.

При работе же через NAT, необходимо настроить TCP-окно на _каждой_ рабочей станции, подключенной к локальной сети.

===== медленный старт и выборочное подтверждение =====

Для предотвращения перегрузок сети в протокол TCP был введен так называемый "**медленный старт**" ("slow start"), подробно описанный в RFC 1122 и RFC 2581.

При создании нового TCP/IP соединения система устанавливает размер окна равный одному сегменту. После получения подтверждения размер окна увеличивается вдвое и так продолжается вплоть до достижения максимально возможного размера.

Экспоненциальный рост ширины окна "съедает" совсем немного времени при передачи огромных файлов, но вот при установке множества TCP/IP соединений (характерных, например, для браузеров), обменивающихся крошечными порциями данных (классический пример которых web-сервер), медленный старт заметно снижает эффективность широких каналов, кроме того, даже при кратковременной перегрузки сети, система сбрасывает размер окна в единицу, в результате чего, график скорости отдачи файла из степной равнины превращается в холмистую терраформу (см. рис. 5).

{{tcp-optimize_Image_4.png}}

**Рисунок 5 "медленный старт" и его последствия (CW – размер окна в сегментах)**

Кроме того, система поддерживает специальный параметр Slow Start Threshold Size (Пороговый Размер [окна] Медленного Старта), по умолчанию равный 65636, но после распознавания ситуации "перегрузка сети", принимающий значение W/2 и в дальнейшем является верхней границей экспоненциального роста параметра CW, что вызывает драматическое падение производительности (см. рис. 6).

{{tcp-optimize_Image_5.png}}

**Рисунок 6 уменьшение размеров TCP-окна при обнаружении перегрузки сети**

Непосредственно отключить "медленный старт" штатными средствами Windows (не прибегая к патчу ядра) нельзя, однако, если задействовать SACK-алгоритм (Selective Acknowledgement — Выборочное подтверждение, одно из расширений TCP-протокола, описанное в RFC 2018), "медленный старт" вырубается сам собой, становясь при этом никому не нужным избыточным пережитком старины.

Выборочное подтверждение передачи позволяет осуществлять повторную передачу неподтвержденных сегментов в одном окне (при неактивном SACK'е потерянные сегменты передаются один за другим в индивидуальном порядке). Другими словами, узел "А" повторно передает узлу "B" только реально потерянные сегменты, а не весь блок, в состав которого входят и успешно принятые пакеты.Очевидно, что максимальный прирост производительности будет наблюдаться на нестабильных каналах связи, регулярно теряющих пакеты.

Для активации алгоритма SACK достаточно установить параметр реестра **SackOpts** в значение "1" (значение по умолчанию для W2K и XP).

===== время, работающее против нас =====

С подтвержденным сегментами все ясно. Если подтверждение пришло, сегмент можно считать успешно доставленным. Весь вопрос в том, сколько это самое подтверждение ждать и когда начинать повторную пересылку.

По умолчанию, Windows Server 2003 ждет три секунды (при желании это значение можно изменить редактированием параметра **TcpInitialRTT**), после чего осуществляет повторную посылку неподтвержденных пакетов, а сам интервал ожидания увеличивают в соответствии с алгоритмом SRTT (Smoothed Round Trip Time — сглаженное оцененное время обращения). Максимальное количество повторных передач хранится в параметре **TcpMaxDataRetransmissions** (по умолчанию равному пяти), при достижении которого соединение разрывается.

Очевидно, что на нестабильных каналах, страдающих хроническими задержками, количество разрывов соединений можно сократить путем увеличения параметра **TcpMaxDataRetransmissions** до любой разумной величины (но не больше FFFFFFFFh). С другой стороны, для повышения производительности и "нейтрализации" пагубного влияния потерянных пакетов, на быстрых каналах с малым временем задержки значение TcpInitialRTT рекомендуется уменьшить до одной секунды.

Главный недостаток статического таймера в его неспособности реагировать на кратковременные изменения характеристик канала связи. Выбранное системой время ожидания подтверждения оказывается то мало, то велико. Производительность падает, пользователь рвет и мечет, а пропускная способность "плавает" в очень широких пределах, заметно отставая от ожидаемой.

Задержанное подтверждение (Delayed Acknowledgement) — еще одно расширение протокола TCP/IP, описанное в RFC 1122 и впервые реализованное в W2K (а так же в NT 4.0 SP4). Вместо того, чтобы подтверждать каждый полученный сегмент, узел "B" теперь отправляет подтверждение только в случае, если в течении определенного промежутка времени (хранящегося в параметре **TcpDelAckTicks** и по умолчанию равном 200 ms), от узла "A" не было получено ни одного сегмента. Другими словами, если сегменты идут дружными косяками и все работает нормально подтверждения не отправляются до тех пор, пока в сети не возникнет "затор". Немного подождав, узел "B" высылает подтверждение о всех полученных сегментах, давая узлу "A" возможность самостоятельно разобраться — какие сегменты потерялись в дороге и передать их повторно с минимальными накладными расходами.

К сожалению, задержка, выбранная компанией Microsoft по умолчанию, близко к латентности сетей с большими задержками, что сводит на нет все достоинства данного алгоритма и для повышения производительности значение TcpDelAckTicks рекомендуется увеличить в несколько раз. Соответственно, на низколатентных сетях, его лучше уменьшить, ликвидируя никому не нужные простои.

Значения данного параметра могут варьироваться в диапазоне от 0 до 6, выражаемом в десятых долях секунды, т.е. единица соответствует 100 ms, а нуль трактуется как запрещает на использование задержанных подтверждений.

При использовании TCP-окон большого размера, рекомендуется задействовать алгоритм временных меток (TCP-Timestamps), описанный в RFC 1323, и автоматически адаптирующий значение таймера повторной передачи даже в условиях быстро меняющихся характеристик канала связи. За это отвечает параметр Tcp1323Opts, который будучи установленным в значение 3 разрешает использование всех расширений RFC 1323.

===== заключение =====

В статье рассмотрены лишь некоторые опции TCP/IP-протокола, в наибольшей степени ответственные за его производительность. Но помимо них существует и другие, за разъяснением которых мы оправляем читателя по ссылкам по врезке.

===== >>> врезка полезные ссылки =====

# Оптимизация работы протокола ТСР в распределенных сетях

http://www.gurnov.ru/kms_catalog+stat+cat_id-4+page-1+nums-14.html;

# Enabling High Performance Data Transfers

http://www.psc.edu/networking/projects/tcptune/

# Step-by-step instructions for tuning TCP under Windows

http://www.psc.edu/networking/projects/tcptune/OStune/winxp/winxp_stepbystep.html

# UNIX IP Stack Tuning Guide

http://www.cymru.com/Documents/ip-stack-tuning.html

# Navas Cable Modem/DSL Tuning Guide

http://cable-dsl.home.att.net

# Microsoft Windows 2000 TCP/IP Implementation Details

http://www.microsoft.com/technet/network/deploy/depovg/tcpip2k.mspx

# TCP/IP and NBT configuration parameters for Windows 2000 or for Windows NT

http://support.microsoft.com/kb/120642/

# PMTU black hole detection algorithm change for Windows

http://support.microsoft.com/kb/136970/) 

# Default MTU size for different network topology

http://support.microsoft.com/kb/140375/) 

# Dial-Up and Home Networking Troubleshooting Reference

http://www.internetweekly.org/llarrow/mtumss.html


